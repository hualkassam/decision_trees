{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "First, let's make sure this notebook works well in both python 2 and 3, import a few common modules, ensure \n",
    "MatplotLib plots figures inline and prepare a function to save the figures\n",
    "'''\n",
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"decision_trees\"\n",
    "\n",
    "def image_path(fig_id):\n",
    "    return os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(image_path(fig_id) + \".png\", format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Visualizing a Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nThen you can convert this .dot file to a variety of formats such as PDF or PNG using the dot command-\\nline tool from the graphviz package. This command line converts the .dot file to a .png image file \\n'dot -Tpng iris_tree.dot -oiris_tree.png'\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "iris = load_iris()\n",
    "X = iris.data[:, 2:] # petal length and width\n",
    "y = iris.target\n",
    "tree_clf = DecisionTreeClassifier(max_depth=2)\n",
    "tree_clf.fit(X, y)\\\n",
    "\n",
    "'''\n",
    "You can visualize the trained Decision Tree by first using the export_graphviz() method to output a graph \n",
    "definition file called iris_tree.dot\n",
    "'''\n",
    "from sklearn.tree import export_graphviz\n",
    "export_graphviz(\n",
    "        tree_clf,\n",
    "        out_file=image_path(\"iris_tree.dot\"),\n",
    "        feature_names=iris.feature_names[2:],\n",
    "        class_names=iris.target_names,\n",
    "        rounded=True,\n",
    "        filled=True\n",
    "    )\n",
    "\n",
    "'''\n",
    "Then you can convert this .dot file to a variety of formats such as PDF or PNG using the dot command-\n",
    "line tool from the graphviz package. This command line converts the .dot file to a .png image file \n",
    "'dot -Tpng iris_tree.dot -oiris_tree.png'\n",
    "'''\n",
    "\n",
    "'''\n",
    "One of the many qualities of Decision Trees is that they require very little data preparation. In particular, \n",
    "they don’t require feature scaling or centering at all.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNOTE: Other algorithms work by first training the Decision Tree without restrictions, then pruning (deleting) \\nunnecessary nodes. A node whose children are all leaf nodes is considered unnecessary if the purity improvement\\nit provides is not statistically significant. Standard statistical tests, such as the χ2 test, are used to \\nestimate the probability that the improvement is purely the result of chance (which is called the null \\nhypothesis). If this probability, called the p-value, is higher than a given threshold (typically 5%,\\ncontrolled by a hyperparameter), then the node is considered unnecessary and its children are deleted. The \\npruning continues until all unnecessary nodes have been pruned.\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Decision Trees make very few assumptions about the training data (as opposed to linear models, which\n",
    "obviously assume that the data is linear, for example). If left unconstrained, the tree structure will adapt\n",
    "itself to the training data, fitting it very closely, and most likely overfitting it. Such a model is often \n",
    "called a nonparametric model, not because it does not have any parameters (it often has a lot) but because the\n",
    "number of parameters is not determined prior to training, so the model structure is free to stick closely to\n",
    "the data. In contrast, a parametric model such as a linear model has a predetermined number of\n",
    "parameters, so its degree of freedom is limited, reducing the risk of overfitting (but increasing the risk of\n",
    "underfitting).\n",
    "\n",
    "To avoid overfitting the training data, you need to restrict the Decision Tree’s freedom during training. As\n",
    "you know by now, this is called regularization. The regularization hyperparameters depend on the algorithm\n",
    "used, but generally you can at least restrict the maximum depth of the Decision Tree. In Scikit-Learn, this is\n",
    "controlled by the max_depth hyperparameter (the default value is None , which means unlimited). Reducing \n",
    "max_depth will regularize the model and thus reduce the risk of overfitting. The DecisionTreeClassifier class\n",
    "has a few other parameters that similarly restrict the shape of the Decision Tree: min_samples_split \n",
    "(the minimum number of samples a node must have before it can be split), min_samples_leaf (the minimum number \n",
    "of samples a leaf node must have), min_weight_fraction_leaf (same as min_samples_leaf but expressed as a \n",
    "fraction of the total number of weighted instances), max_leaf_nodes (maximum number of leaf nodes), and \n",
    "max_features (maximum number of features that are evaluated for splitting at each node). Increasing min_*\n",
    "hyperparameters or reducing max_* hyperparameters will regularize the model.\n",
    "'''\n",
    "'''\n",
    "NOTE: Other algorithms work by first training the Decision Tree without restrictions, then pruning (deleting) \n",
    "unnecessary nodes. A node whose children are all leaf nodes is considered unnecessary if the purity improvement\n",
    "it provides is not statistically significant. Standard statistical tests, such as the χ2 test, are used to \n",
    "estimate the probability that the improvement is purely the result of chance (which is called the null \n",
    "hypothesis). If this probability, called the p-value, is higher than a given threshold (typically 5%,\n",
    "controlled by a hyperparameter), then the node is considered unnecessary and its children are deleted. The \n",
    "pruning continues until all unnecessary nodes have been pruned.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=2, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "           splitter='best')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Decision Trees are also capable of performing regression tasks. Let’s build a regression tree using Scikit-\n",
    "Learn’s DecisionTreeRegressor class, training it on a noisy quadratic dataset with max_depth=2\n",
    "'''\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "tree_reg = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg.fit(X,\ty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
